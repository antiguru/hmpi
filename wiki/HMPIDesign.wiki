#summary HMPI Code Design

HMPI is a relatively small codebase, consisting of the following files:

|| File Name || Purpose ||
|| [https://code.google.com/p/hmpi/source/browse/hmpi/error.h error.h] || Macros for warning and error reporting. ||
|| [https://code.google.com/p/hmpi/source/browse/hmpi/lock.h lock.h] || Platform-specific primitives for locking and related atomic operations. ||
|| [https://code.google.com/p/hmpi/source/browse/hmpi/hmpi.h hmpi.h] || User-visible header file defining (H)MPI interface. ||
|| [https://code.google.com/p/hmpi/source/browse/hmpi/hmpi.c hmpi.c] || Initialization, shutdown, and communicator management. ||
|| [https://code.google.com/p/hmpi/source/browse/hmpi/hmpi_p2p.c hmpi_p2p.c] || Point-to-point message matching, message passing, and ownership passing (give/take). ||
|| [https://code.google.com/p/hmpi/source/browse/hmpi/hmpi_opi.c hmpi_opi.c] || Ownership Passing initialization and memory pools (alloc/free). ||

Related but non-HMPI-specific files:

profile.h        See [PerformanceProfiling]

sm_malloc.c  See [SharedHeapAllocator]

malloc.c        See [SharedHeapAllocator]


= [https://code.google.com/p/hmpi/source/browse/hmpi/error.h error.h] =

Contains two macros, {{{ERROR}}} and {{{WARNING}}}, used throughout the HMPI code.  {{{WARNING}}} prints a message prefixed with the file and line number from where it is called.  {{{ERROR}}} does the same, but then immediately aborts the program.


= [https://code.google.com/p/hmpi/source/browse/hmpi/lock.h lock.h]  =

Contains platform-specific implementation of an atomic lock (mutex), and some other atomic primitives:

|| Function || Purpose ||
|| STORE_FENCE || Ensure all prior memory writes are complete. ||
|| LOAD_FENCE  || Ensure all prior memory reads are complete. ||
|| FENCE            || Ensure all prior memory reads and writes are complete. ||
|| CAS_PTR_BOOL || Compare-and-swap on a pointer value, evaluating to a boolean expression. ||
|| CAS_PTR_VAL || Compare-and-swap evaluating to the previous value of the pointer. ||
|| CAS_T_BOOL || Compare-and-swap an arbitrary type, evaluating to a boolean expression. ||
|| FETCH_ADD32 || Fetch-and-add 32-bit integer value. ||
|| FETCH_ADD64 || Fetch-and-add 64-but integer value. ||
|| FETCH_STORE || Fetch-and-store a pointer value. ||
|| LOCK_INIT || Initialize a lock (mutex). ||
|| LOCK_ACQUIRE || Spin (block without yield) until a lock is acquired. ||
|| LOCK_RELEASE || Release a lock. ||
 
Many of these routines are modeled after (and often implemented by) the [http://gcc.gnu.org/onlinedocs/gcc-4.3.5/gcc/Atomic-Builtins.html Intel/GCC built-in atomic primtives].  In addition to ICC and GCC, they are also supported by XLC on Blue Gene/Q.

On x86, the lock routines are implemented using the well-known [MCS lock algorithms].  These have slightly better scalability (i.e., better performance when many cores are spinning to acquire the lock) than a simple spin-lock, and provide a FIFO ordering guarantee.  That is, if process A gets in line to acquire the lock before process B, process A will always acquire the lock first.  Without FIFO ordering, we found that HMPI performance varied widely due to long delays occasionally caused by lock starvation.

On Blue Gene/Q we found no benefit to using the MCS lock algorithms, so instead we use the PowerPC A2-specific L2 lock primitives, which are faster than the standard PowerPC {{{lwarx}}}/{{{stwcx}}} instructions.


= [https://code.google.com/p/hmpi/source/browse/hmpi/hmpi.h hmpi.h] =

This is the main HMPI header file.  Applications should include {{{hmpi.h}}} instead of the standard {{{mpi.h}}} when compiling to use HMPI.  All of the standard MPI routines, types, and constants implemented by HMPI are designated with an {{{HMPI_}}} prefix.  For each of the MPI routines HMPI implements, a corresponding macro renaming the MPI call to the HMPI call.

Collectives are currently handled differently, as we do not implement any of them directly.  Instead, there is one macro for each MPI collective which passes the call down to the underlying MPI, dereferencing the HMPI communicator object properly along the way.

HMPI defines its own versions of user-visible MPI objects:  {{{HMPI_Comm}}}, {{{HMPI_COMM_WORLD}}}, {{{HMPI_Request}}}, {{{HMPI_Status}}}, etc.

Several internal data structures are also defined here, as they are referenced by the public-facing MPI objects.  {{{HMPI_Item}}} is a simple base type for using objects as linked list elements.  For any structure that will be used as a linked list, its first member variable will be an instance of {{{HMPI_Item}}}.  This allows type casting back and forth between {{{HMPI_Item}}} and the appropriate structure type.  {{{HMPI_Request_list}}} implements a linked list of {{{HMPI_Item}}} objects/elements.  Elements are single-linked while both a head and tail are kept track of.

Again for internal use, a number of macros are defined.  The most interesting is {{{IS_SM_BUF()}}}, which takes a pointer as an argument and evaluates to a boolean indicating whether the specified memory is part of the shared memory heap or not.  Each {{{HMPI_Request}}} has a type associated with it, defined by macros to indicate send/recv, HMPI (local) / MPI (remote), and receives with {{{MPI_ANY_SOURCE}}}.  In addition, each request has a state, either {{{ACTIVE}}} (message is in transit) or {{{COMPLETE}}} (message completed).  Finally, each request has a flag ({{{do_free}}}) for tracking what should be done with its associated message buffer when completed.  {{{DO_NOT_FREE}}} indicates a user buffer that should not be touched, while {{{DO_FREE}}} indicates a regular buffer that should be {{{free}}}'d.  {{{OPI_FREE}}} indicates the buffer should be returned to the memory pool via {{{OPI_Free}}}.


= [https://code.google.com/p/hmpi/source/browse/hmpi/hmpi.c hmpi.c] =

Contains HMPI initialization and communicator management code.  {{{HMPI_Init}}} calls {{{MPI_Init}}}, then sets up {{{HMPI_COMM_WORLD}}}.  Next, it allocates data structures representing incoming local messages and local receives.  Rank 0 on a node does the initialization, then shares the address of the data structures using node-local MPI broadcast.  A final world-wide barrier ensures all ranks have finished initializing before returning to the application.

{{{HMPI_Finalize}}} has little work to do other than print any profiling variables and call {{{MPI_Finalize}}}.

{{{init_communicator}}} does the work of creating an {{{HMPI_Comm}}}, given an {{{MPI_Comm}}}.  This routine fills in the HMPI-specific data structure elements.  Most importantly, for each communicator, we internally split it into sub-communicators, one for each node.  Various fields in the {{{HMPI_Comm}}} object are filled in based on this node-local communicator and are used for quick access to rank/size information throughout the HMPI codebase.

HMPI implements the MPI communicator management routines using {{{init_communicator}}} and calling the equivalent MPI routines.